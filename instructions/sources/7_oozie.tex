\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[polish]{babel}

\begin{document}

Najpierw przekonfiguruj yarna:
\begin{enumerate}
\item Na cloudera managerze wejdź w konfigurację yarna.
\item Wpisz w wyszukiwarkę: yarn.nodemanager.resource.cpu-vcores i ustaw ten parametr na 2.
\item yarn.app.mapreduce.am.resource.mb: 512MiB
\item mapreduce.map.memory.mb: 512MiB
\item mapreduce.reduce.memory.mb: 512MiB
\item yarn.scheduler.minimum-allocation-mb: 512MiB
\item yarn.scheduler.increment-allocation-mb: 512MiB
\item yarn.scheduler.maximum-allocation-mb: 512MiB
\item mapreduce.map.java.opts.max.heap: 400 MiB
\item mapreduce.reduce.java.opts.max.heap: 400 MiB
\item 'ApplicationMaster Java Maximum Heap Size': 400 MiB
\item Kliknij save changes
\item Kliknij w logo cloudera managera w lewym górnym rogu. Przy yarnie pojawiła się ikonka z tooltipem: 'Stale configuration ...'. kliknij ją
\item Kliknij przycisk: 'Restart stale services'
\end{enumerate}

\pagebreak

Job hadoop streaming na oozie:
\begin{enumerate}
\item wrzuć mapper.py i reducer.py na hdfsa
\item rozwiń na hue zakładkę 'Workflows' i następnie kliknij w 'Editors'
\item kliknij po prawej stronie przycisk 'create'
\item Przeciągnij akcję streaming do workflowu
\item W pole Mapper wpisz 'mapper.py', a w pole Reducer wpisz 'reducer.py'
\item Kliknij 'Add'
\item Kliknij w przycisk 'FILES+' i znajdź plik 'mapper.py', następnie kliknij jeszcze raz i znajdź plik 'reducer.py'
\item Kliknij w prawym górnym rogu akcji w przycisk ustawień
\item Kliknij w przycisk 'PROPERTIES+' i wpisz w pierwsze pole 'mapred.input.dir', a w drugim podaj ścieżkę do katalogu / pliku wejściowego
\item Kliknij jeszcze raz i dodaj property 'mapred.output.dir' z namiarami na katalog wyjściowy
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\pagebreak

Job mapreduce:
\begin{enumerate}
\item wrzuć jara z jobem na hdfsa
\item dodaj akcję 'Java program' do workflowu
\item w polu 'Jar name' znajdź jara z jobem na hdfsie
\item w polu 'Main class' wpisz nazwę klasy razem z pakietem
\item Kliknij 2 razy w 'ARGUMENTS+'. W pierwszym polu wpisz plik/katalog wejściowy, a w drugim wyjściowy
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\pagebreak

Job sqoop
\begin{enumerate}
\item wrzuć 'mysql-connector-java-5.1.39-bin.jar' na hdfsa
\item dodaj akcję 'sqoop1' do workflowu
\item w polu "Sqoop command" wpisz komendę do importu sqoop (bez polecenia sqoop) i kliknij add
\item Kliknij w prawym górnym rogu akcji w przycisk ustawień
\item kliknij na 'ARCHIVES+'
\item znajdź na 'hdfsie mysql-connector-java-5.1.39-bin.jar'
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\pagebreak

Job hive
\begin{enumerate}
\item stwórz skrypt 'nazwa.sql' zawierający komenty sqlowe do wykonania
\item umieść skrypt 'nazwa.sql' na hdfsie
\item dodaj akcję 'HiveServer2' do workflowu
\item podaj namiary na skrypt 'nazwa.sql' i kliknij 'add'
\item Zapisz workflow klikając save w prawym górnym rogu
\end{enumerate}

\pagebreak

Koordynator
\begin{enumerate}
\item Kliknij na dropdown 'Workflows'
\item Najedź na 'Editors'
\item Kliknij na 'Coordinators'
\item Kliknij przycisk 'Create' po prawej
\item Kliknij 'Choose a workflow' i wybierz workflow
\item Wybierz jak często job ma się wykonywać
\item kliknij save, żeby zapisać koordynatora
\end{enumerate}

\pagebreak
Zadania:
\begin{enumerate}
\item Utwórz workflow z joba streaming sliczającego słowa i joba mapreduce sortującego po liczbie wystąpień
\item Utwórz workflow ściągający dane ze sqoopa, ładujący je do tabeli na hivie i wyliczający tabelę wynikową będącą połączeniem apache_logs z tabelą ip_name
\end{enumerate}

\end{document}
