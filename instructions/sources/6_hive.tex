\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{polski}
\usepackage[polish]{babel}
\usepackage{listings}
\lstset{language=SQL}

\begin{document}
\pagenumbering{gobble}
\section*{Hive}

\begin{enumerate}
\item zaloguj się na vm-cluster-node1
\item wykonaj polecenie "mysql -u root"
\item wykonaj polecenie 'create database hue;'
\item wykonaj polecenie "grant all on hue.* to 'hue'@'localhost' identified by 'hue';"
\item wejdź w konfigurację serwisu hue na cloudera managerze i kliknij w filtrach na database
\item zmień 'Hue Database Type' na 'mysql'
\item 'Hue Database Hostname' na 'localhost'
\item 'Hue Database Port' na '3306'
\item 'Hue Database Password' na 'hue'
\item zrestartuj serwis hue
\end{enumerate}

\pagebreak

Dokumentacja hive:
\\*
https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DML
\\*
\\*
Beeline connection string:
\\*
!connect jdbc:hive2://vm-cluster-node1:10000 vagrant vagrant
\\*
\\*
Przykłady zapytań:
\begin{lstlisting}
CREATE TABLE IF NOT EXISTS apache_logs(
        ip STRING,
        minus1 STRING,
        minus2 STRING,
        date STRING,
        request STRING,
        response STRING,
        content_lenght INT,
        referer STRING,
        useragent STRING)
    COMMENT 'apache logs'
    ROW FORMAT DELIMITED
    FIELDS TERMINATED BY '|'
    STORED AS TEXTFILE
    LOCATION '/user/vagrant/apache_logs_table';
    
    SHOW CREATE TABLE apache_logs;	
\end{lstlisting}
\begin{lstlisting}
CREATE TABLE orc_logs LIKE apache_logs STORED AS orc;
CREATE TABLE avro_logs STORED AS avro AS select * FROM apache_logs;
\end{lstlisting}

\pagebreak

\begin{lstlisting}
CREATE external TABLE IF NOT EXISTS partitioned_logs (
		ip STRING,
		minus1 STRING,
		minus2 STRING,
		date STRING,
		request STRING,
		response STRING,
		content_length INT,
		referer STRING,
		useragent STRING)
	PARTITIONED BY (method STRING)
	ROW FORMAT DELIMITED
	FIELDS TERMINATED BY '|'
	STORED AS TEXTFILE;
        
SET hive.exec.dynamic.partition.mode=nonstrict;
INSERT INTO partitioned_logs partition(method)
	SELECT *, method_udf(request) AS method FROM apache_logs;        
\end{lstlisting}

\begin{lstlisting}
CREATE FUNCTION hello AS 'pl.isa.hadoop.HelloWorldUdf'
	USING JAR 'hdfs:///user/vagrant/hive-udfs-1.0-SNAPSHOT.jar'
\end{lstlisting}

\pagebreak

Zadania:
\begin{enumerate}
\item Utwórz tabelę apache\_logs. Wrzuć do niej dane z pliku apache\_logs. Możesz skorzystać z polecenia ‘SHOW CREATE TABLE’, żeby namierzyć gdzie hive utworzył bazę na hdfsie i przekopiować plik do katalogu.
\item Utwórz tabelę typu ‘external’ I załaduj do niej takie same dane. Możesz skorzystać z polecenia 'insert into \textit{tabela} select ...'.
\item Zdropuj obie tabele i zobacz co stało się z danymi na hdfsie.
\item Utwórz tabelę w formacie ORC – na końcu polecenia ‘CREATE’ dodaj ‘STORED AS ORC’. Załaduj do niej dane z tabeli apache\_logs. Spróbuj poleceniem ‘hdfs dfs -cat ...’ wypisać zawartość plików i zobacz, że są binarne.
\item Posumuj content\_length po metodzie (PUT, GET) i zobacz, które requesty zajęły najwięcej transferu. Możesz skorzystać z metody hive’owej split(\textit{string}, \textit{pattern}), żeby wyciągnąć metodę z pola request.
\item Utwórz tabelę ip\_name z logów załadowanych sqoopem
\item Utwórz tabelę name\_useragent będącą wynikiem zjoinowania tabel ip\_name i apache\_logs
\item Utwórz UDF, który wyciągnie z pola request metodę i spróbuj go użyć zamiast polecenia split. Jara z udfem najprościej umieścić na hdfsie i dodać do hive’a poleceniem CREATE FUNCTION \textit{nazwa\_funkcji} AS \textit{pakiet.NazwaKlasy} USING JAR ‘hdfs://\textit{ścieżka\_na\_hdfs}’
\item Utwórz tabelę partycjonowaną po metodzie i wrzuć do niej dane z tabeli apache\_logs. Skorzystaj z polecenia SET hive.exec.dynamic.partition=true, żeby odblokować dynamiczne wyznaczanie partycji.
\item Utwórz tabelę partycjonowaną po log\_time z eventów, które wrzucił flume
\end{enumerate}

\end{document}
